{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Anti Money Laundering Detection with GNN node classification\n","### This notenook includes GNN model training and dataset implementation with PyG library. In this example, we used HI-Small_Trans.csv as our dataset for training and testing.  "]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-07T05:03:44.040372Z","iopub.status.busy":"2023-10-07T05:03:44.039793Z","iopub.status.idle":"2023-10-07T05:03:58.147392Z","shell.execute_reply":"2023-10-07T05:03:58.146038Z","shell.execute_reply.started":"2023-10-07T05:03:44.040343Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import datetime\n","import os\n","from typing import Callable, Optional\n","import pandas as pd\n","from sklearn import preprocessing\n","import numpy as np\n","import torch\n","\n","from torch_geometric.data import (\n","    Data,\n","    InMemoryDataset\n",")\n","\n","pd.set_option('display.max_columns', None)\n","path = '/Users/owhy/Documents/Datasets/HI-Small_Trans_3.csv'\n","df = pd.read_csv(path)"]},{"cell_type":"markdown","metadata":{},"source":["# Data visualization and possible feature engineering\n","Let's look into the dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T05:33:22.849494Z","iopub.status.busy":"2023-10-07T05:33:22.849049Z","iopub.status.idle":"2023-10-07T05:33:22.867501Z","shell.execute_reply":"2023-10-07T05:33:22.866198Z","shell.execute_reply.started":"2023-10-07T05:33:22.849448Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["          Timestamp  From Bank    Account  To Bank  Account.1  \\\n","0  2022/09/01 00:20         10  8000EBD30       10  8000EBD30   \n","1  2022/09/01 00:20       3208  8000F4580        1  8000F5340   \n","2  2022/09/01 00:00       3209  8000F4670     3209  8000F4670   \n","3  2022/09/01 00:02         12  8000F5030       12  8000F5030   \n","4  2022/09/01 00:06         10  8000F5200       10  8000F5200   \n","\n","   Amount Received Receiving Currency  Amount Paid Payment Currency  \\\n","0          3697.34          US Dollar      3697.34        US Dollar   \n","1             0.01          US Dollar         0.01        US Dollar   \n","2         14675.57          US Dollar     14675.57        US Dollar   \n","3          2806.97          US Dollar      2806.97        US Dollar   \n","4         36682.97          US Dollar     36682.97        US Dollar   \n","\n","  Payment Format  Is Laundering  \n","0   Reinvestment              0  \n","1         Cheque              0  \n","2   Reinvestment              0  \n","3   Reinvestment              0  \n","4   Reinvestment              0  \n"]}],"source":["print(df.head())"]},{"cell_type":"markdown","metadata":{},"source":["After the viewing the dataframe, we suggest that we can extract all accounts from receiver and payer among all transcation for sorting the suspicious accounts. We can transform the whole dataset into node classification problem by considering accounts as nodes while transcation as edges."]},{"cell_type":"markdown","metadata":{},"source":["The object columns should be encoded into classes with sklearn LabelEncoder."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T05:36:52.790986Z","iopub.status.busy":"2023-10-07T05:36:52.790429Z","iopub.status.idle":"2023-10-07T05:36:52.797831Z","shell.execute_reply":"2023-10-07T05:36:52.796721Z","shell.execute_reply.started":"2023-10-07T05:36:52.790952Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Timestamp              object\n","From Bank               int64\n","Account                object\n","To Bank                 int64\n","Account.1              object\n","Amount Received       float64\n","Receiving Currency     object\n","Amount Paid           float64\n","Payment Currency       object\n","Payment Format         object\n","Is Laundering           int64\n","dtype: object\n"]}],"source":["print(df.dtypes)"]},{"cell_type":"markdown","metadata":{},"source":["Check if there are any null values"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T05:40:11.526713Z","iopub.status.busy":"2023-10-07T05:40:11.526397Z","iopub.status.idle":"2023-10-07T05:40:12.913554Z","shell.execute_reply":"2023-10-07T05:40:12.912335Z","shell.execute_reply.started":"2023-10-07T05:40:11.526687Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Timestamp             0\n","From Bank             0\n","Account               0\n","To Bank               0\n","Account.1             0\n","Amount Received       0\n","Receiving Currency    0\n","Amount Paid           0\n","Payment Currency      0\n","Payment Format        0\n","Is Laundering         0\n","dtype: int64\n"]}],"source":["print(df.isnull().sum())"]},{"cell_type":"markdown","metadata":{},"source":["There are two columns representing paid and received amount of each transcation, wondering if it is necessary to split the amount into two columns when they shared the same value, unless there are transcation fee/transcation between different currency. Let's find out "]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T05:45:40.568327Z","iopub.status.busy":"2023-10-07T05:45:40.567898Z","iopub.status.idle":"2023-10-07T05:45:40.594713Z","shell.execute_reply":"2023-10-07T05:45:40.593358Z","shell.execute_reply.started":"2023-10-07T05:45:40.568296Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Amount Received equals to Amount Paid:\n","False\n","Receiving Currency equals to Payment Currency:\n","False\n"]}],"source":["print('Amount Received equals to Amount Paid:')\n","print(df['Amount Received'].equals(df['Amount Paid']))\n","print('Receiving Currency equals to Payment Currency:')\n","print(df['Receiving Currency'].equals(df['Payment Currency']))"]},{"cell_type":"markdown","metadata":{},"source":["It seens involved the transcations between different currency, let's print it out"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T05:46:16.614934Z","iopub.status.busy":"2023-10-07T05:46:16.614531Z","iopub.status.idle":"2023-10-07T05:46:17.289425Z","shell.execute_reply":"2023-10-07T05:46:17.288314Z","shell.execute_reply.started":"2023-10-07T05:46:16.614907Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["             Timestamp  From Bank    Account  To Bank  Account.1  \\\n","1173  2022/09/01 00:22       1362  80030A870     1362  80030A870   \n","\n","      Amount Received Receiving Currency  Amount Paid Payment Currency  \\\n","1173            52.11               Euro        61.06        US Dollar   \n","\n","     Payment Format  Is Laundering  \n","1173            ACH              0  \n","---------------------------------------------------------------------------\n","             Timestamp  From Bank    Account  To Bank  Account.1  \\\n","1173  2022/09/01 00:22       1362  80030A870     1362  80030A870   \n","\n","      Amount Received Receiving Currency  Amount Paid Payment Currency  \\\n","1173            52.11               Euro        61.06        US Dollar   \n","\n","     Payment Format  Is Laundering  \n","1173            ACH              0  \n"]}],"source":["not_equal1 = df.loc[~(df['Amount Received'] == df['Amount Paid'])]\n","not_equal2 = df.loc[~(df['Receiving Currency'] == df['Payment Currency'])]\n","print(not_equal1)\n","print('---------------------------------------------------------------------------')\n","print(not_equal2)"]},{"cell_type":"markdown","metadata":{},"source":["The size of two df shows that there are transcation fee and transcation between different currency, we cannot combine/drop the amount columns."]},{"cell_type":"markdown","metadata":{},"source":["As we are going to encode the columns, we have to make sure that the classes of same attribute are aligned.\n","Let's check if the list of Receiving Currency and Payment Currency are the same"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T05:51:06.994519Z","iopub.status.busy":"2023-10-07T05:51:06.994058Z","iopub.status.idle":"2023-10-07T05:51:07.455980Z","shell.execute_reply":"2023-10-07T05:51:07.454722Z","shell.execute_reply.started":"2023-10-07T05:51:06.994490Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['Bitcoin', 'Euro', 'US Dollar']\n","['Bitcoin', 'Euro', 'US Dollar']\n"]}],"source":["print(sorted(df['Receiving Currency'].unique()))\n","print(sorted(df['Payment Currency'].unique()))"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preprocessing\n","### We will show the functions used in the PyG dataset first, dataset and model training will be provided in bottom section"]},{"cell_type":"markdown","metadata":{},"source":["In the data preprocessing, we perform below transformation:  \n","1. Transform the Timestamp with min max normalization.  \n","2. Create unique ID for each account by adding bank code with account number.  \n","3. Create receiving_df with the information of receiving accounts, received amount and currency\n","4. Create paying_df with the information of payer accounts, paid amount and currency\n","5. Create a list of currency used among all transactions\n","6. Label the 'Payment Format', 'Payment Currency', 'Receiving Currency' by classes with sklearn LabelEncoder\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T05:53:11.423289Z","iopub.status.busy":"2023-10-07T05:53:11.422843Z","iopub.status.idle":"2023-10-07T05:53:11.432504Z","shell.execute_reply":"2023-10-07T05:53:11.431355Z","shell.execute_reply.started":"2023-10-07T05:53:11.423245Z"},"trusted":true},"outputs":[],"source":["def df_label_encoder(df, columns):\n","        le = preprocessing.LabelEncoder()\n","        for i in columns:\n","            df[i] = le.fit_transform(df[i].astype(str))\n","        return df\n","\n","def preprocess(df):\n","        df = df_label_encoder(df,['Payment Format', 'Payment Currency', 'Receiving Currency'])\n","        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n","        df['Timestamp'] = df['Timestamp'].apply(lambda x: x.value)\n","        df['Timestamp'] = (df['Timestamp']-df['Timestamp'].min())/(df['Timestamp'].max()-df['Timestamp'].min())\n","\n","        df['Account'] = df['From Bank'].astype(str) + '_' + df['Account']\n","        df['Account.1'] = df['To Bank'].astype(str) + '_' + df['Account.1']\n","        df = df.sort_values(by=['Account'])\n","        receiving_df = df[['Account.1', 'Amount Received', 'Receiving Currency']]\n","        paying_df = df[['Account', 'Amount Paid', 'Payment Currency']]\n","        receiving_df = receiving_df.rename({'Account.1': 'Account'}, axis=1)\n","        currency_ls = sorted(df['Receiving Currency'].unique())\n","\n","        return df, receiving_df, paying_df, currency_ls"]},{"cell_type":"markdown","metadata":{},"source":["Let's have a look of processed df"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T05:53:15.266963Z","iopub.status.busy":"2023-10-07T05:53:15.266592Z","iopub.status.idle":"2023-10-07T05:53:56.218064Z","shell.execute_reply":"2023-10-07T05:53:56.216975Z","shell.execute_reply.started":"2023-10-07T05:53:15.266935Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["      Timestamp  From Bank         Account  To Bank        Account.1  \\\n","67     0.172414       1047  1047_800416A40    15723  15723_8051B9F40   \n","3614   0.965517       1047  1047_800679ED0     1047   1047_800679ED0   \n","3702   0.620690       1047  1047_800683140     1047   1047_800683140   \n","3689   0.965517       1047  1047_8006920D0     1047   1047_8006920D0   \n","3812   0.655172       1047  1047_8006B2FA0     1047   1047_8006B2FA0   \n","\n","      Amount Received  Receiving Currency  Amount Paid  Payment Currency  \\\n","67             130.10                   2       130.10                 2   \n","3614        261353.23                   2    261353.23                 2   \n","3702         55345.22                   2     55345.22                 2   \n","3689        181317.26                   2    181317.26                 2   \n","3812         47062.42                   2     47062.42                 2   \n","\n","      Payment Format  Is Laundering  \n","67                 4              0  \n","3614               5              0  \n","3702               5              0  \n","3689               5              0  \n","3812               5              0  \n"]}],"source":["df, receiving_df, paying_df, currency_ls = preprocess(df = df)\n","print(df.head())"]},{"cell_type":"markdown","metadata":{},"source":["paying df and receiving df:"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T05:57:25.918744Z","iopub.status.busy":"2023-10-07T05:57:25.918280Z","iopub.status.idle":"2023-10-07T05:57:25.929797Z","shell.execute_reply":"2023-10-07T05:57:25.928625Z","shell.execute_reply.started":"2023-10-07T05:57:25.918708Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              Account  Amount Received  Receiving Currency\n","67    15723_8051B9F40           130.10                   2\n","3614   1047_800679ED0        261353.23                   2\n","3702   1047_800683140         55345.22                   2\n","3689   1047_8006920D0        181317.26                   2\n","3812   1047_8006B2FA0         47062.42                   2\n","             Account  Amount Paid  Payment Currency\n","67    1047_800416A40       130.10                 2\n","3614  1047_800679ED0    261353.23                 2\n","3702  1047_800683140     55345.22                 2\n","3689  1047_8006920D0    181317.26                 2\n","3812  1047_8006B2FA0     47062.42                 2\n"]}],"source":["print(receiving_df.head())\n","print(paying_df.head())"]},{"cell_type":"markdown","metadata":{},"source":["currency_ls:"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T05:57:28.907031Z","iopub.status.busy":"2023-10-07T05:57:28.906667Z","iopub.status.idle":"2023-10-07T05:57:28.913761Z","shell.execute_reply":"2023-10-07T05:57:28.912327Z","shell.execute_reply.started":"2023-10-07T05:57:28.907004Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[0, 1, 2]\n"]}],"source":["print(currency_ls)"]},{"cell_type":"markdown","metadata":{},"source":["We would like to extract all unique accounts from payer and receiver as node of our graph. It includes the unique account ID, Bank code and the label of 'Is Laundering'.  \n","In this section, we consider both payer and receiver involved in a illicit transaction as suspicious accounts, we will label both accounts with 'Is Laundering' == 1."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T05:57:31.850839Z","iopub.status.busy":"2023-10-07T05:57:31.850459Z","iopub.status.idle":"2023-10-07T05:57:31.858990Z","shell.execute_reply":"2023-10-07T05:57:31.857826Z","shell.execute_reply.started":"2023-10-07T05:57:31.850810Z"},"trusted":true},"outputs":[],"source":["def get_all_account(df):\n","        ldf = df[['Account', 'From Bank']]\n","        rdf = df[['Account.1', 'To Bank']]\n","        suspicious = df[df['Is Laundering']==1]\n","        s1 = suspicious[['Account', 'Is Laundering']]\n","        s2 = suspicious[['Account.1', 'Is Laundering']]\n","        s2 = s2.rename({'Account.1': 'Account'}, axis=1)\n","        suspicious = pd.concat([s1, s2], join='outer')\n","        suspicious = suspicious.drop_duplicates()\n","\n","        ldf = ldf.rename({'From Bank': 'Bank'}, axis=1)\n","        rdf = rdf.rename({'Account.1': 'Account', 'To Bank': 'Bank'}, axis=1)\n","        df = pd.concat([ldf, rdf], join='outer')\n","        df = df.drop_duplicates()\n","\n","        df['Is Laundering'] = 0\n","        df.set_index('Account', inplace=True)\n","        df.update(suspicious.set_index('Account'))\n","        df = df.reset_index()\n","        return df"]},{"cell_type":"markdown","metadata":{},"source":["Take a look of the account list:"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T05:57:34.379521Z","iopub.status.busy":"2023-10-07T05:57:34.378456Z","iopub.status.idle":"2023-10-07T05:57:41.317058Z","shell.execute_reply":"2023-10-07T05:57:41.316062Z","shell.execute_reply.started":"2023-10-07T05:57:34.379481Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["          Account  Bank  Is Laundering\n","0  1047_800416A40  1047              0\n","1  1047_800679ED0  1047              0\n","2  1047_800683140  1047              0\n","3  1047_8006920D0  1047              0\n","4  1047_8006B2FA0  1047              0\n"]}],"source":["accounts = get_all_account(df)\n","print(accounts.head())"]},{"cell_type":"markdown","metadata":{},"source":["# Node features\n","For node features, we would like to aggregate the mean of paid and received amount with different types of currency as the new features of each node. "]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T05:57:43.694369Z","iopub.status.busy":"2023-10-07T05:57:43.693958Z","iopub.status.idle":"2023-10-07T05:57:43.701141Z","shell.execute_reply":"2023-10-07T05:57:43.699901Z","shell.execute_reply.started":"2023-10-07T05:57:43.694334Z"},"trusted":true},"outputs":[],"source":["def paid_currency_aggregate(currency_ls, paying_df, accounts):\n","        for i in currency_ls:\n","            temp = paying_df[paying_df['Payment Currency'] == i]\n","            accounts['avg paid '+str(i)] = temp['Amount Paid'].groupby(temp['Account']).transform('mean')\n","        return accounts\n","\n","def received_currency_aggregate(currency_ls, receiving_df, accounts):\n","    for i in currency_ls:\n","        temp = receiving_df[receiving_df['Receiving Currency'] == i]\n","        accounts['avg received '+str(i)] = temp['Amount Received'].groupby(temp['Account']).transform('mean')\n","    accounts = accounts.fillna(0)\n","    return accounts"]},{"cell_type":"markdown","metadata":{},"source":["Now we can define the node attributes by the bank code and the mean of paid and received amount with different types of currency."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T05:57:45.915808Z","iopub.status.busy":"2023-10-07T05:57:45.915112Z","iopub.status.idle":"2023-10-07T05:57:45.926442Z","shell.execute_reply":"2023-10-07T05:57:45.924963Z","shell.execute_reply.started":"2023-10-07T05:57:45.915734Z"},"trusted":true},"outputs":[],"source":["def get_node_attr(currency_ls, paying_df,receiving_df, accounts):\n","        node_df = paid_currency_aggregate(currency_ls, paying_df, accounts)\n","        node_df = received_currency_aggregate(currency_ls, receiving_df, node_df)\n","        node_label = torch.from_numpy(node_df['Is Laundering'].values).to(torch.float)\n","        node_df = node_df.drop(['Account', 'Is Laundering'], axis=1)\n","        node_df = df_label_encoder(node_df,['Bank'])\n","#         node_df = torch.from_numpy(node_df.values).to(torch.float)  # comment for visualization\n","        return node_df, node_label"]},{"cell_type":"markdown","metadata":{},"source":["Take a look of node_df:"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T05:57:48.258031Z","iopub.status.busy":"2023-10-07T05:57:48.257639Z","iopub.status.idle":"2023-10-07T05:57:56.275657Z","shell.execute_reply":"2023-10-07T05:57:56.274417Z","shell.execute_reply.started":"2023-10-07T05:57:48.257999Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["   Bank  avg paid 0  avg paid 1  avg paid 2  avg received 0  avg received 1  \\\n","0     3         0.0         0.0     3697.34             0.0             0.0   \n","1     3         0.0         0.0        0.01             0.0             0.0   \n","2     3         0.0         0.0    14675.57             0.0             0.0   \n","3     3         0.0         0.0     2806.97             0.0             0.0   \n","4     3         0.0         0.0    36682.97             0.0             0.0   \n","\n","   avg received 2  \n","0         3697.34  \n","1            0.01  \n","2        14675.57  \n","3         2806.97  \n","4        36682.97  \n"]}],"source":["node_df, node_label = get_node_attr(currency_ls, paying_df,receiving_df, accounts)\n","print(node_df.head())"]},{"cell_type":"markdown","metadata":{},"source":["# Edge features\n","In terms of edge features, we would like to conside each transcation as edges.  \n","For edge index, we replace all account with index and stack into a list with size of [2, num of transcation]  \n","For edge attributes, we used 'Timestamp', 'Amount Received', 'Receiving Currency', 'Amount Paid', 'Payment Currency' and 'Payment Format'\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T05:58:06.006625Z","iopub.status.busy":"2023-10-07T05:58:06.006227Z","iopub.status.idle":"2023-10-07T05:58:06.015211Z","shell.execute_reply":"2023-10-07T05:58:06.014356Z","shell.execute_reply.started":"2023-10-07T05:58:06.006594Z"},"trusted":true},"outputs":[],"source":["def get_edge_df(accounts, df):\n","        accounts = accounts.reset_index(drop=True)\n","        accounts['ID'] = accounts.index\n","        mapping_dict = dict(zip(accounts['Account'], accounts['ID']))\n","        df['From'] = df['Account'].map(mapping_dict)\n","        df['To'] = df['Account.1'].map(mapping_dict)\n","        df = df.drop(['Account', 'Account.1', 'From Bank', 'To Bank'], axis=1)\n","\n","        edge_index = torch.stack([torch.from_numpy(df['From'].values), torch.from_numpy(df['To'].values)], dim=0)\n","\n","        df = df.drop(['Is Laundering', 'From', 'To'], axis=1)\n","\n","#         edge_attr = torch.from_numpy(df.values).to(torch.float)  # comment for visualization\n","\n","        edge_attr = df  # for visualization\n","        return edge_attr, edge_index"]},{"cell_type":"markdown","metadata":{},"source":["edge_attr:"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T06:00:02.820037Z","iopub.status.busy":"2023-10-07T06:00:02.819644Z","iopub.status.idle":"2023-10-07T06:00:07.880960Z","shell.execute_reply":"2023-10-07T06:00:07.879754Z","shell.execute_reply.started":"2023-10-07T06:00:02.820005Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["      Timestamp  Amount Received  Receiving Currency  Amount Paid  \\\n","67     0.172414           130.10                   2       130.10   \n","3614   0.965517        261353.23                   2    261353.23   \n","3702   0.620690         55345.22                   2     55345.22   \n","3689   0.965517        181317.26                   2    181317.26   \n","3812   0.655172         47062.42                   2     47062.42   \n","\n","      Payment Currency  Payment Format  \n","67                   2               4  \n","3614                 2               5  \n","3702                 2               5  \n","3689                 2               5  \n","3812                 2               5  \n"]}],"source":["edge_attr, edge_index = get_edge_df(accounts, df)\n","print(edge_attr.head())"]},{"cell_type":"markdown","metadata":{},"source":["edge_index:"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T05:58:16.265617Z","iopub.status.busy":"2023-10-07T05:58:16.265045Z","iopub.status.idle":"2023-10-07T05:58:16.274597Z","shell.execute_reply":"2023-10-07T05:58:16.273471Z","shell.execute_reply.started":"2023-10-07T05:58:16.265571Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[   0,    1,    2,  ..., 3732, 3732, 3733],\n","        [3734,    1,    2,  ..., 3732, 3732, 4187]])\n"]}],"source":["print(edge_index)"]},{"cell_type":"markdown","metadata":{},"source":["# Final code \n","### Below we will show the final code for model.py, train.py and dataset.py"]},{"cell_type":"markdown","metadata":{},"source":["# Model Architecture\n","In this section, we used Graph Attention Networks as our backbone model.  \n","The model built with two GATConv layers followed by a linear layer with sigmoid outout for classification"]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch_geometric.transforms as T\n","from torch_geometric.nn import GATConv, Linear\n","\n","class GAT(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n","        super().__init__()\n","        self.conv1 = GATConv(in_channels, hidden_channels, heads, dropout=0.6)\n","        self.conv2 = GATConv(hidden_channels * heads, int(hidden_channels/4), heads=1, concat=False, dropout=0.6)\n","        self.lin = Linear(int(hidden_channels/4), out_channels)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x, edge_index, edge_attr):\n","        x = F.dropout(x, p=0.6, training=self.training)\n","        x = F.elu(self.conv1(x, edge_index, edge_attr))\n","        x = F.dropout(x, p=0.6, training=self.training)\n","        x = F.elu(self.conv2(x, edge_index, edge_attr))\n","        x = self.lin(x)\n","        x = self.sigmoid(x)\n","        \n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["# PyG InMemoryDataset\n","Finally we can build the dataset with above functions"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[],"source":["class AMLtoGraph(InMemoryDataset):\n","\n","    def __init__(self, root: str, edge_window_size: int = 10,\n","                 transform: Optional[Callable] = None,\n","                 pre_transform: Optional[Callable] = None):\n","        self.edge_window_size = edge_window_size\n","        super().__init__(root, transform, pre_transform)\n","        self.data, self.slices = torch.load(self.processed_paths[0])\n","\n","    @property\n","    def raw_file_names(self) -> str:\n","        return 'HI-Small_Trans_3.csv'\n","\n","    @property\n","    def processed_file_names(self) -> str:\n","        return 'data.pt'\n","\n","    @property\n","    def num_nodes(self) -> int:\n","        return self._data.edge_index.max().item() + 1\n","\n","    def df_label_encoder(self, df, columns):\n","        le = preprocessing.LabelEncoder()\n","        for i in columns:\n","            df[i] = le.fit_transform(df[i].astype(str))\n","        return df\n","\n","\n","    def preprocess(self, df):\n","        df = self.df_label_encoder(df,['Payment Format', 'Payment Currency', 'Receiving Currency'])\n","        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n","        df['Timestamp'] = df['Timestamp'].apply(lambda x: x.value)\n","        df['Timestamp'] = (df['Timestamp']-df['Timestamp'].min())/(df['Timestamp'].max()-df['Timestamp'].min())\n","\n","        df['Account'] = df['From Bank'].astype(str) + '_' + df['Account']\n","        df['Account.1'] = df['To Bank'].astype(str) + '_' + df['Account.1']\n","        df = df.sort_values(by=['Account'])\n","        receiving_df = df[['Account.1', 'Amount Received', 'Receiving Currency']]\n","        paying_df = df[['Account', 'Amount Paid', 'Payment Currency']]\n","        receiving_df = receiving_df.rename({'Account.1': 'Account'}, axis=1)\n","        currency_ls = sorted(df['Receiving Currency'].unique())\n","\n","        return df, receiving_df, paying_df, currency_ls\n","\n","    def get_all_account(self, df):\n","        ldf = df[['Account', 'From Bank']]\n","        rdf = df[['Account.1', 'To Bank']]\n","        suspicious = df[df['Is Laundering']==1]\n","        s1 = suspicious[['Account', 'Is Laundering']]\n","        s2 = suspicious[['Account.1', 'Is Laundering']]\n","        s2 = s2.rename({'Account.1': 'Account'}, axis=1)\n","        suspicious = pd.concat([s1, s2], join='outer')\n","        suspicious = suspicious.drop_duplicates()\n","\n","        ldf = ldf.rename({'From Bank': 'Bank'}, axis=1)\n","        rdf = rdf.rename({'Account.1': 'Account', 'To Bank': 'Bank'}, axis=1)\n","        df = pd.concat([ldf, rdf], join='outer')\n","        df = df.drop_duplicates()\n","\n","        df['Is Laundering'] = 0\n","        df.set_index('Account', inplace=True)\n","        df.update(suspicious.set_index('Account'))\n","        df = df.reset_index()\n","        return df\n","    \n","    def paid_currency_aggregate(self, currency_ls, paying_df, accounts):\n","        for i in currency_ls:\n","            temp = paying_df[paying_df['Payment Currency'] == i]\n","            accounts['avg paid '+str(i)] = temp['Amount Paid'].groupby(temp['Account']).transform('mean')\n","        return accounts\n","\n","    def received_currency_aggregate(self, currency_ls, receiving_df, accounts):\n","        for i in currency_ls:\n","            temp = receiving_df[receiving_df['Receiving Currency'] == i]\n","            accounts['avg received '+str(i)] = temp['Amount Received'].groupby(temp['Account']).transform('mean')\n","        accounts = accounts.fillna(0)\n","        return accounts\n","\n","    def get_edge_df(self, accounts, df):\n","        accounts = accounts.reset_index(drop=True)\n","        accounts['ID'] = accounts.index\n","        mapping_dict = dict(zip(accounts['Account'], accounts['ID']))\n","        df['From'] = df['Account'].map(mapping_dict)\n","        df['To'] = df['Account.1'].map(mapping_dict)\n","        df = df.drop(['Account', 'Account.1', 'From Bank', 'To Bank'], axis=1)\n","\n","        edge_index = torch.stack([torch.from_numpy(df['From'].values), torch.from_numpy(df['To'].values)], dim=0)\n","\n","        print(edge_index)\n","\n","        df = df.drop(['Is Laundering', 'From', 'To'], axis=1)\n","\n","        edge_attr = torch.from_numpy(df.values).to(torch.float)\n","        return edge_attr, edge_index\n","\n","    def get_node_attr(self, currency_ls, paying_df,receiving_df, accounts):\n","        node_df = self.paid_currency_aggregate(currency_ls, paying_df, accounts)\n","        node_df = self.received_currency_aggregate(currency_ls, receiving_df, node_df)\n","        node_label = torch.from_numpy(node_df['Is Laundering'].values).to(torch.float)\n","        node_df = node_df.drop(['Account', 'Is Laundering'], axis=1)\n","        node_df = self.df_label_encoder(node_df,['Bank'])\n","        node_df = torch.from_numpy(node_df.values).to(torch.float)\n","        return node_df, node_label\n","\n","    def process(self):\n","        df = pd.read_csv(self.raw_paths[0])\n","        df, receiving_df, paying_df, currency_ls = self.preprocess(df)\n","        accounts = self.get_all_account(df)\n","        node_attr, node_label = self.get_node_attr(currency_ls, paying_df,receiving_df, accounts)\n","        edge_attr, edge_index = self.get_edge_df(accounts, df)\n","\n","        data = Data(x=node_attr,\n","                    edge_index=edge_index,\n","                    y=node_label,\n","                    edge_attr=edge_attr\n","                    )\n","        \n","        data_list = [data] \n","        if self.pre_filter is not None:\n","            data_list = [d for d in data_list if self.pre_filter(d)]\n","\n","        if self.pre_transform is not None:\n","            data_list = [self.pre_transform(d) for d in data_list]\n","\n","        data, slices = self.collate(data_list)\n","        torch.save((data, slices), self.processed_paths[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Model Training \n","As we cannot create folder in kaggle, please follow the instructions in https://github.com/issacchan26/AntiMoneyLaunderingDetectionWithGNN before you start training "]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["dataset = AMLtoGraph('/Users/owhy/Documents/Datasets')\n","data = dataset[0]"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Data(x=[4188, 7], edge_index=[2, 4999], edge_attr=[4999, 6], y=[4188])\n"]}],"source":["print(data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[   0,    1,    2,  ..., 3732, 3732, 3733],\n","        [3734,    1,    2,  ..., 3732, 3732, 4187]])"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["edge_index"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[   0,    1,    2,  ..., 3732, 3732, 3733],\n","        [3734,    1,    2,  ..., 3732, 3732, 4187]])"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["data.edge_index"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'torch_geometric.data.data.Data'>\n"]}],"source":["print(type(data))\n"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[{"data":{"text/plain":["7"]},"execution_count":103,"metadata":{},"output_type":"execute_result"}],"source":["data.num_features # TODO node features"]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[{"data":{"text/plain":["GAT(\n","  (conv1): GATConv(7, 16, heads=8)\n","  (conv2): GATConv(128, 4, heads=1)\n","  (lin): Linear(4, 1, bias=True)\n","  (sigmoid): Sigmoid()\n",")"]},"execution_count":112,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([0., 0., 0.,  ..., 0., 0., 0.])"]},"execution_count":113,"metadata":{},"output_type":"execute_result"}],"source":["data.y "]},{"cell_type":"code","execution_count":97,"metadata":{"trusted":true},"outputs":[{"ename":"ImportError","evalue":"'NeighborSampler' requires either 'pyg-lib' or 'torch-sparse'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[97], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 35\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch_geometric/loader/node_loader.py:147\u001b[0m, in \u001b[0;36mNodeLoader.collate_fn\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Samples a subgraph from a batch of input nodes.\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m input_data: NodeSamplerInput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_data[index]\n\u001b[0;32m--> 147\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_sampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_per_worker:  \u001b[38;5;66;03m# Execute `filter_fn` in the worker process\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_fn(out)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:322\u001b[0m, in \u001b[0;36mNeighborSampler.sample_from_nodes\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_from_nodes\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    320\u001b[0m     inputs: NodeSamplerInput,\n\u001b[1;32m    321\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[SamplerOutput, HeteroSamplerOutput]:\n\u001b[0;32m--> 322\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mnode_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubgraph_type \u001b[38;5;241m==\u001b[39m SubgraphType\u001b[38;5;241m.\u001b[39mbidirectional:\n\u001b[1;32m    324\u001b[0m         out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto_bidirectional()\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:542\u001b[0m, in \u001b[0;36mnode_sample\u001b[0;34m(inputs, sample_fn)\u001b[0m\n\u001b[1;32m    539\u001b[0m     seed \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mnode\n\u001b[1;32m    540\u001b[0m     seed_time \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mtime\n\u001b[0;32m--> 542\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43msample_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m out\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m (inputs\u001b[38;5;241m.\u001b[39minput_id, inputs\u001b[38;5;241m.\u001b[39mtime)\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:508\u001b[0m, in \u001b[0;36mNeighborSampler._sample\u001b[0;34m(self, seed, seed_time, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m     num_sampled_nodes \u001b[38;5;241m=\u001b[39m num_sampled_edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 508\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requires \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meither \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyg-lib\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch-sparse\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m SamplerOutput(\n\u001b[1;32m    512\u001b[0m     node\u001b[38;5;241m=\u001b[39mnode,\n\u001b[1;32m    513\u001b[0m     row\u001b[38;5;241m=\u001b[39mrow,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m     num_sampled_edges\u001b[38;5;241m=\u001b[39mnum_sampled_edges,\n\u001b[1;32m    519\u001b[0m )\n","\u001b[0;31mImportError\u001b[0m: 'NeighborSampler' requires either 'pyg-lib' or 'torch-sparse'"]}],"source":["import torch\n","import torch_geometric.transforms as T\n","from torch_geometric.loader import NeighborLoader\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","dataset = AMLtoGraph('/Users/owhy/Documents/Datasets')\n","data = dataset[0]\n","epoch = 20\n","\n","model = GAT(in_channels=data.num_features, hidden_channels=16, out_channels=1, heads=8) # TODO node features\n","model = model.to(device)\n","criterion = torch.nn.BCELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n","\n","split = T.RandomNodeSplit(split='train_rest', num_val=0.1, num_test=0)\n","data = split(data)\n","\n","train_loader = loader = NeighborLoader(\n","    data,\n","    num_neighbors=[30] * 2,\n","    batch_size=256,\n","    input_nodes=data.train_mask,\n",")\n","\n","test_loader = loader = NeighborLoader(\n","    data,\n","    num_neighbors=[30] * 2,\n","    batch_size=256,\n","    input_nodes=data.val_mask,\n",")\n","\n","for i in range(epoch):\n","    total_loss = 0\n","    model.train()\n","    for data in train_loader:\n","        optimizer.zero_grad()\n","        data.to(device)\n","        pred = model(data.x, data.edge_index, data.edge_attr) # TODO nodes, adjacency matrix, edge attribute\n","        ground_truth = data.y # TODO True labels\n","        loss = criterion(pred, ground_truth.unsqueeze(1))\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += float(loss)\n","    if epoch%10 == 0: # TODO once training is done --> evaluation\n","        print(f\"Epoch: {i:03d}, Loss: {total_loss:.4f}\")\n","        model.eval()\n","        acc = 0\n","        total = 0\n","        with torch.no_grad():\n","            for test_data in test_loader:\n","                test_data.to(device)\n","                pred = model(test_data.x, test_data.edge_index, test_data.edge_attr)\n","                ground_truth = test_data.y\n","                correct = (pred == ground_truth.unsqueeze(1)).sum().item()\n","                total += len(ground_truth)\n","                acc += correct\n","            acc = acc/total\n","            print('accuracy:', acc)"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Name: torch\n","Version: 2.2.2\n","Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n","Home-page: https://pytorch.org/\n","Author: PyTorch Team\n","Author-email: packages@pytorch.org\n","License: BSD-3\n","Location: /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages\n","Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n","Required-by: torchaudio, torchvision\n"]}],"source":["! pip3 show torch"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: wheel in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.43.0)\n"]}],"source":["! pip3 install wheel\n"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement NeighborSampler (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for NeighborSampler\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["! pip3 install NeighborSampler"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in links: https://data.pyg.org/whl/torch-1.13.0+cpu.html\n","\u001b[31mERROR: Could not find a version that satisfies the requirement pyg_lib (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for pyg_lib\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["! pip3 install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-1.13.0+cpu.html\n"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[{"ename":"SyntaxError","evalue":"incomplete input (2663714873.py, line 1)","output_type":"error","traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[91], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    if (torch_geometric.typing.WITH_PYG_LIB and self.subgraph_type != SubgraphType.induced):\u001b[0m\n\u001b[0m                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"]}],"source":["if (torch_geometric.typing.WITH_PYG_LIB and self.subgraph_type != SubgraphType.induced):\n"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch==2.2.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.2)\n","Collecting torchvision==0.17.2\n","  Downloading torchvision-0.17.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n","Collecting torchaudio==2.2.2\n","  Downloading torchaudio-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\n","Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.2) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.2) (4.11.0)\n","Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.2) (1.12)\n","Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.2) (3.3)\n","Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.2) (3.1.3)\n","Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.2) (2024.3.1)\n","Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchvision==0.17.2) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchvision==0.17.2) (10.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch==2.2.2) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch==2.2.2) (1.3.0)\n","Downloading torchvision-0.17.2-cp312-cp312-macosx_11_0_arm64.whl (1.6 MB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading torchaudio-2.2.2-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: torchvision, torchaudio\n","Successfully installed torchaudio-2.2.2 torchvision-0.17.2\n"]}],"source":["! pip3 install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2\n"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch_geometric in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.5.2)\n","Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch_geometric) (4.66.2)\n","Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch_geometric) (1.26.4)\n","Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch_geometric) (1.12.0)\n","Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch_geometric) (2024.3.1)\n","Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch_geometric) (3.1.3)\n","Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch_geometric) (3.9.5)\n","Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch_geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch_geometric) (3.1.2)\n","Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch_geometric) (1.4.2)\n","Requirement already satisfied: psutil>=5.8.0 in /Users/owhy/Library/Python/3.12/lib/python/site-packages (from torch_geometric) (5.9.8)\n","Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->torch_geometric) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->torch_geometric) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.9.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch_geometric) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->torch_geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->torch_geometric) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->torch_geometric) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->torch_geometric) (2024.2.2)\n","Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn->torch_geometric) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn->torch_geometric) (3.4.0)\n"]}],"source":["! pip3 install torch_geometric\n"]},{"cell_type":"markdown","metadata":{},"source":["# Future Work\n","In this notebook, we performed the node classification with GAT and the result accuracy looks satisfied.  \n","However, it may due to highly imbalance data of the dataset. It is suggested that balance the class of 1 and 0 in the data preprocessing. It is expected that the accuracy will dropped a little bit after balancing the data.  We will keep exploring to see if there are any other models give better performance, such as other traditional regression/classifier model."]},{"cell_type":"markdown","metadata":{},"source":["## Reference\n","Some of the feature engineering of this repo are referenced to below papers, highly recommend to read:\n","1. [Weber, M., Domeniconi, G., Chen, J., Weidele, D. K. I., Bellei, C., Robinson, T., & Leiserson, C. E. (2019). Anti-money laundering in bitcoin: Experimenting with graph convolutional networks for financial forensics. arXiv preprint arXiv:1908.02591.](https://arxiv.org/pdf/1908.02591.pdf)\n","2. [Johannessen, F., & Jullum, M. (2023). Finding Money Launderers Using Heterogeneous Graph Neural Networks. arXiv preprint arXiv:2307.13499.](https://arxiv.org/pdf/2307.13499.pdf)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
