{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Data Preprocessing\n",
    "- Using the IBM dataset for AML: https://www.kaggle.com/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml\n",
    "- Dataset is generated by IBM Box Generator, models transactions and illicit activities.\n",
    "- Original dataset for training will be extensively large, for initial stages of thesis, using smaller dataset of 500,000 transactions.\n",
    "- In the following we will:\n",
    "1. explore the data\n",
    "2. determine nodes and edges\n",
    "3. determine node and edge attributes\n",
    "4. create visualization using NetworkX, PyVis, or Graph-tool\n",
    "\n",
    "* Attributes on ACCOUNT\n",
    "    * Bank account\n",
    "    * Account balance\n",
    "    * BIN number\n",
    "    * Number of transactions (calculated)\n",
    "    * Receiving Currency\n",
    "* Attributes on TRANSACTIONS\n",
    "    * Payment amount\n",
    "    * Payment Type\n",
    "    * Payment Currency (based on “receiving currency” of outgoing bank account)\n",
    "    * Time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- IMPORT LIBRARIES --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install torch numpy pandas matplotlib torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from pandas import Timestamp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import to_networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- LOAD DATASET --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/owhy/Documents/Datasets/HI-Small_Trans.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(filename)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"---- shape ----\\n - {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---- info ----\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---- basic calculations ----\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NODE MATRIX\n",
    "\n",
    "Nodes = Bank Accounts -- bank account number\n",
    "* BIN Number\n",
    "* Receiving Currency\n",
    "\n",
    "* Number of transactions (degree matrix --> calculated based on incoming and outcoming flows)\n",
    "\n",
    "Edges = transactions -- payment amount\n",
    "* Payment Type\n",
    "* Payment Currency\n",
    "* Date and Time\n",
    "\n",
    "* Account Balance (before transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merging all accounts and getting unique values\n",
    "\n",
    "# Merge \"Account\" and \"Account.1\" columns\n",
    "merged_accounts = pd.concat([data['Account'], data['Account.1']])\n",
    "\n",
    "# Merge \"From Bank\" and \"To Bank\" columns\n",
    "merged_banks = pd.concat([data['From Bank'], data['To Bank']])\n",
    "\n",
    "# Merge \"Receiving Currency\" and \"Payment Currency\" columns\n",
    "merged_currencies = pd.concat([data['Receiving Currency'], data['Payment Currency']])\n",
    "\n",
    "# Create a new DataFrame with merged columns\n",
    "merged_df = pd.DataFrame({\n",
    "    'Accounts': merged_accounts,\n",
    "    'Bank': merged_banks,\n",
    "    'Currency': merged_currencies\n",
    "})\n",
    "\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on the \"Accounts\" column to ensure uniqueness\n",
    "unique_accounts = merged_df.drop_duplicates(subset=['Accounts']).reset_index(drop=True)\n",
    "unique_accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- One-hot encoding: currency ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert non-numeric columns\n",
    "positions = unique_accounts[\"Currency\"].str.split(\",\", expand=True) # creating new columns by splitting receiving currency --> all are added\n",
    "unique_accounts[\"first_position\"] = positions[0] # first currency in each row is extracted --> actual currency used and that we want as TRUE\n",
    "# One-hot encoding \n",
    "node_features = pd.concat([unique_accounts, pd.get_dummies(unique_accounts[\"first_position\"],dtype='int')], axis=1, join='inner') # effectively adds actual currency to dummy variables/columns\n",
    "node_features.drop([\"Currency\", \"first_position\"], axis=1, inplace=True) # drop the axiliary columns\n",
    "node_features.head()\n",
    "\n",
    "# TODO-DONE! conc unnique random number to the end --> maintain uniqueness\n",
    "# TODO-DONE! node --> feature --> feature2 --> node2 | problem with uniqueness of node embeddings --> add unique value to identify the node feature vector\n",
    "# TODO-DONE! create random identity vector for each ACCOUNT + add Account and From Bank as separate properties of the node\n",
    "# TODO-DONE! ultimately normalize From bank\n",
    "# TODO-DONE! feature matrix -- > receiving currency n-hot encoding (0 and 1) + encoding of From Bank and Account (word2vec)\n",
    "# TODO add edge features --> look into EDGE LABELED GRAPHS where all nodes and edges have labels\n",
    "# TODO create init for loading graph --> initial step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Normalization and Vectorization ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize 'From Bank' & 'Account'\n",
    "def normalize(table, new_min=0, new_max=10):\n",
    "    if len(table.columns) == 1:\n",
    "        normalized_df = ((table - table.min()) / (table.max() - table.min())) * (new_max - new_min) + new_min\n",
    "        return normalized_df\n",
    "    else:\n",
    "        normalized_df = pd.DataFrame()\n",
    "        id = 0\n",
    "        for column in table.columns:\n",
    "            col_data = table[column]\n",
    "            if id == 0:\n",
    "                normalized_df[f'col_{id}'] = ((col_data - col_data.min()) / (col_data.max() - col_data.min())) * (new_max - new_min) + new_min\n",
    "            else:\n",
    "                normalized_column = ((col_data - col_data.min()) / (col_data.max() - col_data.min())) * (new_max - new_min) + new_min\n",
    "                normalized_df[f'col_{id}'] = normalized_column\n",
    "            id += 1\n",
    "        # print(normalized_df)\n",
    "        return normalized_df\n",
    "\n",
    "def hashing_vectorization(strings, vector_size=9):\n",
    "    vectors = []\n",
    "    for string in strings:\n",
    "        # Hash the string using hash()\n",
    "        hashed_values = hash(string) % (10 ** vector_size)  # Ensures unique representations within the specified range\n",
    "        \n",
    "        # Convert hashed values to a fixed-size vector\n",
    "        vector = [int(digit) for digit in str(hashed_values)]\n",
    "        \n",
    "        # Ensure vector has the desired size by zero-padding or truncating\n",
    "        if len(vector) < vector_size:\n",
    "            vector = [0] * (vector_size - len(vector)) + vector\n",
    "        elif len(vector) > vector_size:\n",
    "            vector = vector[:vector_size]\n",
    "        \n",
    "        vectors.append(vector)\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_bank_col = node_features.pop('Bank')\n",
    "account_col = node_features.pop('Accounts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node labels for later use\n",
    "\n",
    "node_labels = pd.DataFrame(account_col)\n",
    "node_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add vectors as individuals columns in new dataframe\n",
    "\n",
    "df = pd.DataFrame(account_col, columns=['Accounts'])\n",
    "df.reset_index(drop=True, inplace=True) # Ensure the DataFrame has the same number of rows as the original series\n",
    "vectors = hashing_vectorization(df['Accounts'], vector_size=9) \n",
    "\n",
    "# Convert vectors into DataFrame\n",
    "vectors_df = pd.DataFrame(vectors, columns=[f'col_{i}' for i in range(len(vectors[0]))])\n",
    "result_df = pd.concat([df, vectors_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df = result_df.drop(columns=[\"Accounts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_bank_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_bank_binary = [bin(x).split(\"b\")[1] for x in from_bank_col]\n",
    "# vectors_df = pd.DataFrame(vectors, columns=[f'col_{i}' for i in range(len(vectors[0]))])\n",
    "\n",
    "res = max(from_bank_binary, key=len) \n",
    "print(\"Longest String is  : \", res)\n",
    "len(res)\n",
    "\n",
    "from_bank_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary_fixed_length(binary_lists, res):\n",
    "    new_binary_list = []\n",
    "    for x in binary_lists:\n",
    "        # print(x)\n",
    "        if len(x) < len(res):\n",
    "            num_zeros = len(res) - len(x)\n",
    "            x = [0] * num_zeros + x\n",
    "            # print(x)\n",
    "            new_binary_list.append(x)\n",
    "        else:\n",
    "            new_binary_list.append(x)\n",
    "    return new_binary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_lists = [[int(bit) for bit in binary] for binary in from_bank_binary]\n",
    "binary_lists = make_binary_fixed_length(binary_lists, res)\n",
    "binary_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert vectors into DataFrame\n",
    "bin_vectors_df = pd.DataFrame(binary_lists, columns=[f'bin_{i}' for i in range(len(binary_lists[0]))])\n",
    "bin_vectors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO normalize vector values to avoid big numbers\n",
    "\n",
    "from_bank_df = pd.DataFrame(from_bank_col)\n",
    "accounts_df = pd.DataFrame(accounts_df)\n",
    "\n",
    "# from_bank_df_norm = normalize(from_bank_df,0,1) # TODO do not normalize at this point --> create BINARY representation\n",
    "accounts_df_norm = normalize(accounts_df,0,1)\n",
    "\n",
    "accounts_df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features.reset_index(drop=True, inplace=True) # Ensure the DataFrame has the same number of rows as the original series\n",
    "accounts_df_norm.reset_index(drop=True, inplace=True) # Ensure the DataFrame has the same number of rows as the original series\n",
    "\n",
    "node_features = pd.concat([node_features, accounts_df_norm], axis=1)\n",
    "node_features = pd.concat([node_features, bin_vectors_df], axis=1)\n",
    "node_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Account: Unique Identifier ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add unique random identified\n",
    "\n",
    "unique_ids_set = set()\n",
    "\n",
    "while len(unique_ids_set) < len(node_features): # uniqueness kept\n",
    "    unique_ids_set.add(random.random())\n",
    "\n",
    "unique_ids = list(unique_ids_set)\n",
    "\n",
    "node_features.insert(0, \"Unique ID\", unique_ids)\n",
    "\n",
    "node_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- X = Node Feature Matrix ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO nodes should be bank accounts and not transactions. Bank accounts have unique receiving currencies and \"bank BINs\"\n",
    "x = node_features.to_numpy()\n",
    "x.shape # [num_nodes x num_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDGE MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add edge features --> look into EDGE LABELED GRAPHS where all nodes and edges have labels\n",
    "# TODO create init for loading graph --> initial step\n",
    "# TODO add 2 levels of depth --> will be interconnected, no need to do this step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Finding Links: Mapping ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [{'source': source, 'destination': destination} for source, destination in zip(data['Account'], data['Account.1'])]\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Edge Features ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add edge features --> create matrix like those for nodes\n",
    "\n",
    "edges_df = data[[\"Timestamp\", \"Amount Paid\", \"Payment Currency\", \"Payment Format\"]]\n",
    "edges_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Payment Encoding ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_amount = edges_df[\"Amount Paid\"].astype(str)\n",
    "edges_amount = list(edges_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unused_decimals(number):\n",
    "    # Convert number to string to iterate through digits\n",
    "    num_str = str(number)\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through digits from the end\n",
    "    for digit in reversed(num_str):\n",
    "        # If the digit is '0', increment count\n",
    "        if digit == '0':\n",
    "            count += 1\n",
    "        # If non-zero digit encountered, break the loop\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Remove trailing zeroes from the number\n",
    "    num_str = num_str.rstrip('0')\n",
    "\n",
    "    return num_str, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum = str(max(edges_df[\"Amount Paid\"]))\n",
    "max_len = len(maximum.split(\".\")[0])\n",
    "\n",
    "minimum = min(edges_df[\"Amount Paid\"])\n",
    "minimum = format(minimum, 'f')\n",
    "min_len = len(str(minimum.split('.')[1]))\n",
    "\n",
    "new_min, count = count_unused_decimals(minimum)\n",
    "min_len = min_len - count\n",
    "\n",
    "number_columns = max_len + min_len\n",
    "number_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_vectors(table):\n",
    "    lists = []\n",
    "    for binary in table:\n",
    "        binary = float(binary)\n",
    "        binary = str(format(binary, 'f'))\n",
    "        decimal_repr = []\n",
    "        for bit in binary:\n",
    "            if '.' not in bit:\n",
    "                decimal_repr.append(str(int(bit)))\n",
    "            else:\n",
    "                decimal_repr.append(bit)\n",
    "        lists.append(decimal_repr)\n",
    "    return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_payment_amount(df_col, max_len, min_len):\n",
    "    new_payment_list = []\n",
    "    for x in df_col:\n",
    "        # print(x)\n",
    "        index_of_decimal = x.index('.')\n",
    "        positive_decimals = index_of_decimal\n",
    "        negative_decimals = len(x) - (index_of_decimal+1)\n",
    "        if positive_decimals < max_len:\n",
    "            num_zeros = max_len - positive_decimals\n",
    "            x = ['0'] * num_zeros + x\n",
    "            # print(x)\n",
    "            new_payment_list.append(x)\n",
    "        elif negative_decimals < min_len:\n",
    "            num_zeros = max_len - negative_decimals\n",
    "            x = ['0'] * num_zeros + x\n",
    "            # print(x)\n",
    "            new_payment_list.append(x)\n",
    "        else:\n",
    "            new_payment_list.append(x)\n",
    "        x.remove('.')\n",
    "    return new_payment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = split_into_vectors(edges_amount) # INEFFICIENT !!!! # INEFFICIENT !!!!# INEFFICIENT !!!!# INEFFICIENT !!!!# INEFFICIENT !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_payment_list = encode_payment_amount(a, max_len, min_len) # INEFFICIENT !!!! # INEFFICIENT !!!!# INEFFICIENT !!!!# INEFFICIENT !!!!# INEFFICIENT !!!!\n",
    "new_payment_list = nested_list_int = [[int(item) for item in sublist] for sublist in new_payment_list]\n",
    "new_payment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert vectors into DataFrame\n",
    "payment_vectors_df = pd.DataFrame(new_payment_list, columns=[f'payment_{i}' for i in range(len(new_payment_list[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_features = pd.concat([edges_df, payment_vectors_df], axis=1)\n",
    "edges_features.drop(\"Amount Paid\", axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- One-hot encoding: currency ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO convert Currency into one-hot encoding\n",
    "\n",
    "positions = edges_features[\"Payment Currency\"].str.split(\",\", expand=True) # creating new columns by splitting receiving currency --> all are added\n",
    "edges_features[\"first_position\"] = positions[0] # first currency in each row is extracted --> actual currency used and that we want as TRUE\n",
    "# One-hot encoding \n",
    "edges_features = pd.concat([edges_features, pd.get_dummies(edges_features[\"first_position\"],dtype='int')], axis=1, join='inner') # effectively adds actual currency to dummy variables/columns\n",
    "edges_features.drop([\"Amount Paid\",\"Payment Currency\", \"first_position\"], axis=1, inplace=True) # drop the axiliary columns\n",
    "edges_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO convert Payment Format\n",
    "\n",
    "positions_2 = edges_features[\"Payment Format\"].str.split(\",\", expand=True) \n",
    "edges_features[\"second_position\"] = positions_2[0]\n",
    "edges_features = pd.concat([edges_features, pd.get_dummies(edges_features[\"second_position\"],dtype='int')], axis=1, join='inner') # effectively adds actual currency to dummy variables/columns\n",
    "edges_features.drop([\"Payment Format\", \"second_position\"], axis=1, inplace=True) # drop the axiliary columns\n",
    "edges_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- One-hot encoding: time ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO convert timestamps\n",
    "\n",
    "edges_features[\"Timestamp\"] = pd.to_datetime(edges_features['Timestamp']).astype(int) // 10**9 # does not interpret time well... circular definition for months --> sinus calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Y - Edge Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = edges_features.to_numpy()\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAPHICAL - nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.Graph()\n",
    "\n",
    "transaction_limit = 800 # set a limit for graph visualization\n",
    "\n",
    "for i in range(0,transaction_limit):\n",
    "    u = links[i].get(\"source\")\n",
    "    v = links[i].get(\"destination\")\n",
    "    graph.add_edge(u,v,label=edges_amount[i]) # use edge labels for edge features?\n",
    "\n",
    "# # get all links\n",
    "# for i in range(len(links)):\n",
    "#     u = links[i].get(\"source\")\n",
    "#     v = links[i].get(\"destination\")\n",
    "#     graph.add_edge(u,v,label=edge_attr[i][1]) # use edge labels for edge features?\n",
    "\n",
    "print(graph.edges(data=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Visualization ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.random_layout(graph) # shell, circular, spectral, spring, random, \n",
    "\n",
    "plt.figure(figsize=(25, 15))  # Increase figure size\n",
    "\n",
    "nx.draw(\n",
    "    graph, \n",
    "    pos, \n",
    "    node_size=300,  # Reduce node size for better visibility\n",
    "    with_labels=True, \n",
    "    font_size=7, \n",
    "    font_weight='bold', \n",
    "    node_color='lightblue',  # Specify node color\n",
    "    edge_color='gray',  # Specify edge color\n",
    "    width=1,  # Adjust edge width\n",
    "    arrows=True,  # Show arrows for directed edges\n",
    "    arrowstyle='->',  # Specify arrow style\n",
    "    arrowsize=20,  # Adjust arrow size\n",
    ")\n",
    "\n",
    "edge_labels = nx.get_edge_attributes(graph, 'label')\n",
    "nx.draw_networkx_edge_labels(\n",
    "    graph, \n",
    "    pos, \n",
    "    edge_labels=edge_labels, \n",
    "    label_pos=0.5,  # Adjust label position along edges\n",
    "    font_size=7,  # Adjust font size\n",
    "    font_color='green',  # Specify font color\n",
    ")\n",
    "\n",
    "plt.title(f'Graph Visualization of first {transaction_limit} transactions')  # Add title to the plot\n",
    "plt.axis('off')  # Hide axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries \n",
    "\n",
    "degree_of_centrality = nx.degree_centrality(graph) # closeness_centrality, eigenvector_centrality, betweeness_centrality\n",
    "betweenness_centrality = nx.betweenness_centrality(graph)\n",
    "\n",
    "# TODO add statistics to a new dataframe\n",
    "\n",
    "node_stat_features = pd.DataFrame()\n",
    "node_stat_features['account'] = degree_of_centrality.keys()\n",
    "node_stat_features['degree_of_centrality'] = degree_of_centrality.values()\n",
    "node_stat_features['betweenness_centrality'] = betweenness_centrality.values()\n",
    "\n",
    "node_stat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Statistics as Features - Skip if not needed ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add statistics df to the original df if necessary to include\n",
    "\n",
    "# TODO format decimal point system to exclude power values.\n",
    "\n",
    "# node_stat_features.drop('account')\n",
    "\n",
    "# edges_features_w_stats = pd.concat([node_features, node_stat_features], axis=1)\n",
    "# edges_features_w_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADJACENCY MATRIX - nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Loading full graph ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING FULL GRAPH\n",
    "\n",
    "graph_full = nx.Graph()\n",
    "\n",
    "# get all links\n",
    "for i in range(len(links)):\n",
    "    u = links[i].get(\"source\")\n",
    "    v = links[i].get(\"destination\")\n",
    "    graph_full.add_edge(u,v,label=edges_amount[i]) # use edge labels for edge features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(edges_features))\n",
    "print(len(links)) # number of transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unique_accounts))\n",
    "print(graph_full.__len__()) # number of nodes in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = nx.adjacency_matrix(graph_full)\n",
    "adjacency_matrix = adjacency_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adjacency_matrix) # index of nodes\n",
    "print(len(adjacency_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------- !!! TODO !!! ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is node2vec?\n",
    "\n",
    "# TODO BINs sould also be vectorized to avoid ordering\n",
    "# TODO binary encoding for banks --> replace 300 features with 10 features that can represent a number in binary \n",
    "# TODO take random subset from transactions\n",
    "\n",
    "# TODO calculating time --> circular definition for months (goes back to 0) --> sinus calculations --> same time different year cannot be distinguished in unix format\n",
    "# TODO --> don't do yet.\n",
    "\n",
    "# TODO payment amount --> separate based on power of 10s --> separate columns for thousands, hundreds etc.\n",
    "\n",
    "# TODO no need to add nodes in order --> mapping thorugh dictionary is an option\n",
    "\n",
    "# TODO potentially add statistics to feature matrix X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NEXT 2 WEEKS:\n",
    "\n",
    "1. Vectorize \"BANKS\" - BINs using binary encoding --> DONE!!!\n",
    "2. Vectorize \"Amount Paid\" --> DONE!!!\n",
    "3. Mapping through dictionary of all links --> No need to find individual links between accounts --> we are taking all accounts and the graph in general  --> DONE!!!\n",
    "4. Graph Visualization should include labels that are actual accounts  --> DONE!!!\n",
    "5. Add statistics to feature matrix X? --> if necessary --> DONE!!!\n",
    "6. Apply graph on subset of 1000 transactions  --> DONE!!! --> not a problem at all, efficiency is better.\n",
    "7. Create Adjacency Matrix --> can be done through networkx or through existing links --> DONE!!!\n",
    "7a. Get unique nodes from both \"Account\" and \"Account.1\"  --> DONE!!!\n",
    "\n",
    "7b. If laundering --> make node red\n",
    "\n",
    "8. Create GNN model\n",
    "8a. Create smaller graph\n",
    "8b. Simple GNN, testing on dataset\n",
    "\n",
    "\n",
    "Kernel keeps dying due to low RAM memory, I have to store the graph and variables somewhere else. All I need is the full_graph, x, y, and adjacency matrix.\n",
    "- Save graph and model into Memory\n",
    "- Use cuda\n",
    "\n",
    "9. Write introduction\n",
    "10. Write literature review\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node embedding: ADJACENCY MATRIX * NODE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the GNN architecture\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(input_dim, hidden_dim)\n",
    "        self.gc2 = GraphConvolution(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.relu(self.gc1(x, adj))\n",
    "        x = self.relu(self.gc2(x, adj))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        out = torch.matmul(adj, x)  # Aggregate neighbor information\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "# Prepare data\n",
    "# Assuming x is node features, y is edge features, and adjacency_matrix is the adjacency matrix\n",
    "\n",
    "# Define some constants\n",
    "num_nodes = x.shape[0]\n",
    "input_dim = x.shape[1]\n",
    "output_dim = 1  # Binary classification: fraud or not\n",
    "hidden_dim = 64\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "x = torch.FloatTensor(x)\n",
    "y = torch.FloatTensor(y)\n",
    "adjacency_matrix = torch.FloatTensor(adjacency_matrix)\n",
    "\n",
    "# Split data into train and test sets\n",
    "num_train_samples = 50000\n",
    "train_indices = np.random.choice(num_nodes, num_train_samples, replace=False)\n",
    "test_indices = np.setdiff1d(np.arange(num_nodes), train_indices)\n",
    "\n",
    "x_train, x_test = x[train_indices], x[test_indices]\n",
    "adj_train, adj_test = adjacency_matrix[train_indices][:, train_indices], adjacency_matrix[test_indices][:, test_indices]\n",
    "y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(x_train, adj_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "model = GNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_batch, adj_batch, y_batch = batch\n",
    "        output = model(x_batch, adj_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(x_test, adj_test)\n",
    "    predictions = torch.sigmoid(output).round().squeeze().numpy()\n",
    "\n",
    "# Assuming y_test contains the true labels\n",
    "accuracy = np.mean(predictions == y_test.numpy())\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Learnable Node Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "    \n",
    "    def forward(self, adj_matrix, node_features):\n",
    "        # Convert adjacency matrix to PyTorch tensor\n",
    "        adj_matrix_tensor = torch.from_numpy(adj_matrix).float()\n",
    "        # Normalize adjacency matrix\n",
    "        adj_matrix_normalized = torch.matmul(adj_matrix_tensor, torch.diag(torch.pow(torch.sum(adj_matrix_tensor, dim=1), -0.5)))\n",
    "        # Convert node features to PyTorch tensor\n",
    "        node_features_tensor = torch.from_numpy(node_features).float()\n",
    "        # Compute node embeddings\n",
    "        node_embeddings = torch.matmul(adj_matrix_normalized, node_features_tensor)\n",
    "        # Apply linear transformation\n",
    "        output = self.linear(node_embeddings)\n",
    "        # Apply ReLU activation function\n",
    "        output = F.relu(output)\n",
    "        return output\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layer1 = GCNLayer(num_features, hidden_size)\n",
    "        self.layer2 = GCNLayer(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, adj_matrix, node_features):\n",
    "        h1 = self.layer1(adj_matrix, node_features)\n",
    "        output = self.layer2(adj_matrix, h1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "num_features = 10  # Number of node features\n",
    "hidden_size = 16   # Hidden layer size\n",
    "num_classes = 2    # Number of output classes\n",
    "adj_matrix = torch.randn(10, 10)  # Example adjacency matrix\n",
    "node_features = torch.randn(10, num_features)  # Example node features\n",
    "\n",
    "# Create GCN model\n",
    "model = GCN(num_features, hidden_size, num_classes)\n",
    "\n",
    "# Forward pass\n",
    "output = model(adjacency_matrix, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMPLE GNN MODEL EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nnd1\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features =\n",
    "hidden_dim =\n",
    "num_classes =\n",
    "num_epochs =\n",
    "features =\n",
    "adj =\n",
    "labels ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GCNBlock, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = self.linear(x)\n",
    "        x = torch.matmul(adj, x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gcn1 = GCNBlock(input_dim, hidden_dim)\n",
    "        self.gcn2 = GCNBlock(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = self.gcn1(x, adj)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.gcn2(x, adj)\n",
    "        return x\n",
    "# Define the model\n",
    "model = GCN(num_features, hidden_dim, num_classes)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(features, adj)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
